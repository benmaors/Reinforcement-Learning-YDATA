{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_reinforce_pytorch.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2-FAi-w3pL5"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLv5GceI3pL7",
        "outputId": "d5576d83-f43c-4d99-ab32-a12dff72161a"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzOH27ib3pL8"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJu7-ig43pL8"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "NmCajoUR3pL8",
        "outputId": "dddbc88c-1563-47c8-cf7c-265104305f27"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb8efcf6ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHkZV3EV3pL9"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tt1dZnu3pL9"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUf4ebRv3pL9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBKsYGwd3pL-"
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        " # <YOUR CODE: define a neural network that predicts policy logits>\n",
        " nn.Linear(state_dim[0],32),\n",
        " nn.ReLU(),\n",
        " nn.Linear(32,32),\n",
        " nn.ReLU(),\n",
        " nn.Linear(32,n_actions),\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqCCXKfq3pL-"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g05fG2mq3pL-"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K6DTbF-3pL-"
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    #<YOUR CODE>\n",
        "    states = torch.tensor(states, dtype = torch.float32)\n",
        "    logits = model(states)\n",
        "    \n",
        "    # print(pi)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = nn.functional.softmax(logits,dim=1)\n",
        "\n",
        "    return out.numpy()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64-18WRP3pL_"
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0FeuqhtKtRK",
        "outputId": "ee5ad43c-8b21-4b6f-c0e9-2fb3c441f7f5"
      },
      "source": [
        "print(test_states)\n",
        "print(test_probas)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.03059619 -0.01119172 -0.04304189  0.03148254]\n",
            " [ 0.02585746  0.03799668  0.01076659 -0.00456229]\n",
            " [ 0.01021784  0.03889405 -0.03760297  0.00800929]\n",
            " [ 0.04464545  0.03975308  0.04037162 -0.03027167]\n",
            " [-0.03890202 -0.03482944  0.00638369 -0.03600815]]\n",
            "[[0.49522942 0.5047706 ]\n",
            " [0.4950001  0.50499994]\n",
            " [0.4951986  0.5048014 ]\n",
            " [0.4947929  0.505207  ]\n",
            " [0.4956998  0.5043002 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyEhZerp3pL_"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5-t13M63pL_"
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(2, p=action_probs) #<YOUR CODE>\n",
        "        #print(action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaq77eZs3pL_"
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vrOTnLc3pMA"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3Bqw_d73pMA"
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    G = rewards.copy()\n",
        "\n",
        "    for idx in range(len(G)-2,-1,-1):\n",
        "        G[idx] = rewards[idx] + gamma*G[idx+1]\n",
        "\n",
        "#    <YOUR CODE>\n",
        "    return G #<YOUR CODE: array of cumulative rewards>"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf_zqQl93pMA",
        "outputId": "1b8d1272-5641-413f-c503-d9119a4ce1a9"
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUtc6NNh3pMB"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Oi3jnjc3pMB"
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apztj2KY3pMB"
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "\n",
        "    entropy = torch.sum(probs*log_probs)\n",
        "    J_hat = torch.mean(log_probs_for_actions*cumulative_returns)\n",
        "\n",
        "    loss = -(J_hat + entropy_coef*entropy)\n",
        "    # Gradient descent step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkzKxkSY3pMC"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SODeCTdf3pMC",
        "outputId": "c20e60ab-936c-4b43-ed96-f0e06f2891a6"
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:22.950\n",
            "mean reward:24.870\n",
            "mean reward:36.640\n",
            "mean reward:45.530\n",
            "mean reward:64.300\n",
            "mean reward:82.760\n",
            "mean reward:51.650\n",
            "mean reward:133.700\n",
            "mean reward:138.980\n",
            "mean reward:365.930\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayQmaHa3pMC"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv0I-2QE3pMC"
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.1.62.video000064.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "kM3iTuMC3pMC",
        "outputId": "7d9ed756-d002-450d-b9df-a7fe056e67f4"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.1.62.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVFBsJGp3pMD"
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'your.email@example.com', 'YourAssignmentToken')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkN-gvX33pMD"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ]
}